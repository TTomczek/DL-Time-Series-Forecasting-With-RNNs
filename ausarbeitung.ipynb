{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbc33831cdcd3d27",
   "metadata": {},
   "source": [
    "# Time Series Forecasting mit rekurrenten neuronalen Netzen\n",
    "Dieses Notebook entsteht im Rahmen des Moduls `Deep Learning` an der Fachhochschule Südwestfalen. Es beschäftigt sich mit dem Thema `Time Series Forecasting` und untersucht die Anwendung von rekurrenten neuronalen Netzen (RNN) auf Zeitreihendaten...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec515710907f7ee3",
   "metadata": {},
   "source": [
    "## 1. Einleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dedd5c44055225",
   "metadata": {},
   "source": [
    "## 2. Was ist Time Series Forecasting (Zeitreihenvorhersage)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70f7640-11d0-4c4a-8241-48b0ad583f53",
   "metadata": {},
   "source": [
    "Christian\n",
    "\n",
    "Definition gemäß der Webseite Studysmarter:<br>\n",
    "**Zeitreihenprognose** ist eine statistische Methode zur Vorhersage zukünftiger Ereignisse oder Werte, indem historische Zeitreihendaten analysiert und Muster erkannt werden. [C1]\n",
    "\n",
    "Beispiele:<br>\n",
    "- Wetterdaten für zukünftiges Wetter (https://www.studysmarter.de/studium/mathematik-studium/statistik-studium/zeitreihenprognose/)\n",
    "- Aktienkurse für zukünftige Aktienverläufe\n",
    "- Passagierzahlen für zukünftige Passagiervorhersagen (https://data-science-crashkurs.de/chapters/kapitel_09.html)\n",
    "\n",
    "Die Seite https://www.emft.fraunhofer.de/de/kompetenzen/systemloesungen-ki/ki-algorithmen-zeitreihenanalyse.html nennt zudem noch folgende mögliche Beispiele:\n",
    "- Industrie 4.0 mit IIoT (Industrial Internet of Things)\n",
    "- Logistik\n",
    "- Finanzen\n",
    "- Stromnetze\n",
    "- Internetverkehr\n",
    "- Umweltmonitoring\n",
    "- Gesundheitsdaten\n",
    "\n",
    "Außerdem werden auf der Seite typische Methoden der künstlichen Intelligenz inklusive grafischer Beispiele genannt, wie z.B.: Regression, Klassifikation, Anomalieerkennung, Vorhersage\n",
    "\n",
    "Grafisches Beispiel einer Klassifikation:<br>\n",
    "<img style = 'border: 5px solid #555' src=\"Pictures/classification-dt-mls-emft.jpg\" width=\"30%\" alt=\"Grafisches Beispiel einer Klassifikation\"> \n",
    "<br>\n",
    "Grafisches Beispiel einer Anomalieerkennung:<br>\n",
    "<img style = 'border: 5px solid #555' src=\"Pictures/anomaly-dt-mls-emft.jpg\" width=\"30%\" alt=\"Grafisches Beispiel einer Anomalieerkennung\">\n",
    "<br>\n",
    "Grafisches Beispiel einer Anomalieerkennung:<br>\n",
    "<img style = 'border: 5px solid #555' src=\"Pictures/forecasting-dt-mls-emft.jpg\" width=\"30%\" alt=\"Grafisches Beispiel einer Vorhersage\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46aa77e9-974d-453e-ae40-93c3d9e37a87",
   "metadata": {
    "tags": []
   },
   "source": [
    "Quellen:\n",
    "* C1: StudySmarter GmbH. (2024) Zeitreihenprognose: Methoden & Anwendung | StudySmarter. https://www.studysmarter.de/studium/mathematik-studium/statistik-studium/zeitreihenprognose/. 29.12.2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43de5d59607d0ce7",
   "metadata": {},
   "source": [
    "## 3. Was sind recurrent neural networks (Rekurrente neuronale Netzwerke)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afca4b64d6396f5d",
   "metadata": {},
   "source": [
    "Ein rekurrentes oder auch rückgekoppeltes neuronales Netzwerk (RNN) ist ein künstliches neuronales Netzwerk, dass bei jeder Verarbeitung einer Einheit auch ein Ergebnis aus einer früheren Iteration erhält. Damit eignet es sich besonders für die Verarbeitung von sequentiellen Daten, wie z.B. Zeitreihendaten, Text oder Sprache.\n",
    "Ein RNN funktioniert, indem es eine spezielle Ausgabe des vorherigen Schrittes, den hidden state oder auch state vector, als zusätzlichen Input für den nächsten Schritt verwendet. Das bedeutet, dass immer zwei Ausgaben generiert werden. Dadurch kann es sich an vorherige Iterationen erinnern und somit auch auf längere Abhängigkeiten in den Daten reagieren. Das eigentliche Ergebnis wird dabei nur im letzten Schritt betrachtet. [T1, S. 116ff]\n",
    "\n",
    "<img style = 'border: 5px solid #555' src=\"Pictures/rnn-ablauf.png\" width=\"30%\" alt=\"Ablauf eines RNN\">\n",
    "\n",
    "[T1, S. 117]\n",
    "\n",
    "Das Bild zeigt den sequentiellen Ablauf eines RNN. Dabei bezeichnet \"t\" den aktuellen Schritt. Diese Schritte könnten bei der Verarbeitung von Text z.B. einzelne Wörter oder Buchstaben sein. Der hidden state wird dabei in jedem Schritt aktualisiert und an den nächsten Schritt weitergegeben.\n",
    "\n",
    "Die Verarbeitung einer Interation erfolgt dabei in drei Schritten die in folgendem Bild dargestellt sind:\n",
    "\n",
    "<img style = 'border: 5px solid #555' src=\"Pictures/rnn-schritte.png\" width=\"30%\" alt=\"Ablauf einer RNN Iteration\">\n",
    "\n",
    "[T1, S. 119]\n",
    "\n",
    "Der erste Schritt verbindet die beiden Eingangsvektoren zu einem einzigen Vektor. Anschließend wird der hidden State aktualisiert und im letzten Schritt wird der Ausgabewert berechnet.\n",
    "\n",
    "Das vorangegangene Beispiel lässt sich mit wenigen Zeilen Code in PyTorch implementieren. Bei dem nachfolgenden Code handelt es lediglich um einen Auszug und nicht um ein vollständiges, lauffähiges Beispiel.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size) # input to hidden as linear layer\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size) # input to output as linear layer\n",
    "        self.softmax = nn.LogSoftmax(dim=1) # softmax function for output\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1) # concatenate input and hidden state\n",
    "        hidden = self.i2h(combined) # calculate new hidden state\n",
    "        output = self.i2o(combined) # calculate output\n",
    "        output = self.softmax(output) # apply softmax function\n",
    "        return output, hidden \n",
    "    def initHidden(self):\n",
    "        return Variable(torch.zeros(1, self.hidden_size)) # initialize hidden state with zeros\n",
    "\n",
    "# Run the RNN\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "hidden = rnn.initHidden() # initialize hidden state\n",
    "\n",
    "for i in range(len(toy_story_review)):\n",
    "    output, hidden = rnn(toy_story_review[i], hidden) # process each word in the review\n",
    "```\n",
    "[T1, S. 118]\n",
    "\n",
    "Da ein RNN nun auch eine Zeitkomponente mit bringt, ist der normale Backpropagation Algorithmus nicht anwendbar, da dieser die Schritte des RNN nicht berücksichtigen kann.\n",
    "Die Formel für den Backpropagation Algorithmus lautet:\n",
    "$$ \\Delta w_{ij} = -\\eta \\frac{\\partial E}{\\partial w_{ij}} $$\n",
    "Dabei ist $ \\Delta w_{ij} $ das gewicht zwischen den Neuronen i und j, $ \\eta $ die Lernrate und $ \\partial E $ der Fehler der Fehlerfunktion (Loss).\n",
    "\n",
    "\n",
    "Um dieses Problem zu lösen wurde der \"Backpropagation through time\" Algorithmus entwickelt. Dieser besteht aus vier Schritten:\n",
    "\n",
    "Zuerst wird der Fehler $ \\delta_t $ für jeden Zeitschritt berechnet. Dieser wird mit der Ableitung der Aktivierungsfunktion $ g'(y_t) $ multipliziert. Als aktivierungsfunktion verwendet man üblicherweise die Tanh-Funktion.\n",
    "$$ \\delta_t = \\frac{\\partial E}{\\partial y_t} \\cdot g'(y_t) $$\n",
    "\n",
    "Anschließend wird der Gradient für jeden Zeitschritt berechnet, indem die zuvor berechneten Fehler $ \\delta_t $ mit dem hidden statedes Zeitschritts $ \\cdot h_t^T $ multipliziert und anschließend aufsummiert werden.\n",
    "$$ \\Delta W_y = \\sum_{t} \\delta_t \\cdot h_t^T $$\n",
    "\n",
    "Danach wird der Gradient des hidden state $ \\Delta W_h $ berechnet. Dafür wird der Fehler $ \\delta_t $ mit der Ableitung des hidden state multipliziert und wieder über alle Zeitschritte aufsummiert.\n",
    "$$ \\Delta W_h = \\sum_{t} \\delta_t \\cdot \\frac{\\partial h_t}{\\partial W_h} $$\n",
    "\n",
    "Zuletzt wird der Gradient des Eingangsvektors berechnet. Dies geschieht analog zum vorherigen Schritt, nur dass hier der Fehler $ \\delta_t $ mit der Ableitung des Eingangsvektors $ \\delta W_x $, anstatt dem hidden state $ \\delta W_h $, multipliziert wird.\n",
    "$$ \\Delta W_x = \\sum_{t} \\delta_t \\cdot \\frac{\\partial h_t}{\\partial W_x} $$\n",
    "\n",
    "Diese einfache Form von RNNs hat jedoch zwei Probleme, die als Vanishing und Exploding Gradient Problem bekannt sind. Diese treten auf, wenn eine große Sequenz an Daten verarbeitet werden soll. Bei diesen Problemen werden die Gradienten entweder zu klein oder zu groß, um sinnvolle Änderungen an den Gewichten vornehmen zu können. Dadurch kann das RNN keine langfristigen Abhängigkeiten in den Daten erkennen. Um dieses Problem zu lösen, gibt es verschiedene Ansätze wie zum Beispiel das Gradient Clipping, was die Werte der Gradienten mit einem Maximal- und einem Minimalwert begrenzt, oder der Einsatz von Normalisierungstechniken wie dem Layer Normalization oder dem Batch Normalization. [T2, S. 275ff]\n",
    "In realen Anwendungsszenarien werden jedoch speziell aufgrund dieser Probleme entwickelte Netzwerke wie das Long Short-Term Memory (LSTM) oder das Gated Recurrent Unit (GRU) verwendet. Diese Netzwerke sind in der Lage, langfristige Abhängigkeiten in den Daten zu erkennen und zu verarbeiten.\n",
    "\n",
    "Das Long Short-Term Memory (LSTM) Netzwerk wurde 1997 von Sepp Hochreiter und Jürgen Schmidhuber entwickelt, um die Probleme des einfachen RNN zu lösen.\n",
    "Dabei ist folgende Architektur entstanden:\n",
    "\n",
    " <img style = 'border: 5px solid #555' src=\"Pictures/lstm-zelle.png\" width=\"50%\" alt=\"Architektur eines LSTM\">\n",
    " \n",
    " [T2, S. 407]\n",
    " \n",
    "Oben in der Abbildung ist eine gerade Linie, vom Eingang c(t-1), zu sehen, die mit nur wenigen Schritten Informationen weitergibt. Dies ist das \"Langzeitgedächtnis\" (L) des LSTM. Dieses passiert zuerst das sogenannte \"Forget-Gate\", wo mittels einer elementweisen Multiplikation entschieden wird welche Information vergessen werden soll. Die Entscheidung welche Information vergessen werden soll, wird durch die Sigmoid (logistische) Funktion f(t) entschieden die nach einer linearen Schicht auf die Daten des \"Kurzzeitgedächtnisses\" (S) h(t-1) zusammen mit dne aktuellen Eingabedaten x(t) angewendet wird. Anschließend wird über die Tanh Funktion g(t), wieder nach einer linearen Schicht mit den Daten aus h(t-1) und x(t), entschieden welche neuen Informationen in das Langzeitgedächtnis übernommen werden sollen. Zuvor werden diese Daten noc h durch eine weitere Sigmoid Funktion i(t) gefiltert. Das Langzeitgedächtnis wird ab dieser Stelle nicht mehr verändert und kann ausgegeben werden. Das Kurzzeitgedächtnis durch eine Tanh Funktion und einer Sigmoid Funktion o(t) gefiltert und ausgegeben. Die Sigmoid Funktion o(t) entscheidet dabei welche Informationen aus dem Langzeitgedächtnis ausgegeben werden sollen. Dieses Ergebnis ergibt dann das neue Kurzzeitgedächtnis h(t), dass an den nächsten Schritt weitergegeben werden kann, und das aktuelle Ergebnis y(t). [T2, S. 406ff]\n",
    "\n",
    "Eine neuere Variante des LSTM ist das Gated Recurrent Unit (GRU). Dieses wurde 2014 von Kyunghyun Cho et al. entwickelt und ist eine vereinfachte Version des LSTM. Es besteht aus zwei Gates, dem Reset-Gate (Forget-Gate) und dem Update-Gate (Eingabe-Gate). Das Reset-Gate entscheidet, welche Informationen vergessen werden sollen, und das Update-Gate entscheidet, welche Informationen im hidden state behalten werden sollen. [T2, S. 410]\n",
    "\n",
    "Die Architektur des GRU sieht folgendermaßen aus:\n",
    "\n",
    "<img style = 'border: 5px solid #555' src=\"Pictures/gru-zelle.png\" width=\"50%\" alt=\"Architektur eines GRU\">\n",
    "\n",
    "[T2, S. 410]\n",
    "\n",
    "Im Vergleich zum LSTM ist weniger komplex. Der auffälligste Unterschied ist das Fehlen des Langzeitgedächtnisses. Beim GRU werden Lang- und Kurzzeitgedächtnis in einem hidden state zusammengefasst. Das Reset- und das Update-Gate werden dabei durch einen einzigen Controller, einer linearen Schicht und einer Sigmoid Funktion z(t), gesteuert. Dabei wird das Ergebnis für das Forget-Gate negiert. Wird eine Information hinzugefügt, wird als Gegenteil zuvor eine andere vergessen. Die Informationen die hinzugefügt werden sollen stammen aus einer Tanh Funktion und einer Sigmoid Funktion r(t). Beim GRU entspricht dann der hidden state auch dem Ausgabewert. [T2, S. 410]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Quellen:\n",
    "* T1: Mitchell, L. et al. (2019) Deep learning with Pytorch 1. x: implement deep learning techniques and neural network architecture variants using Python /. Second edition. Birmingham; Packt Publishing.\n",
    "* T2: Géron, A. & Rother, K. (2018) Praxiseinstieg Machine Learning mit Scikit-Learn und TensorFlow: Konzepte, Tools und Techniken für intelligente Systeme. 1. Auflage. Heidelberg: O’Reilly.\n",
    "* T3: Sepp Hochreiter, Jürgen Schmidhuber; Long Short-Term Memory. Neural Comput 1997; 9 (8): 1735–1780. doi: https://doi.org/10.1162/neco.1997.9.8.1735\n"
   ],
   "id": "172359285f1e85b9"
  },
  {
   "cell_type": "markdown",
   "id": "ea2d24091bb38610",
   "metadata": {},
   "source": "## 4. Datenbeschaffung"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Für die folgenden Implementierungen verschiedener RNNs wird ein Datensatz der Plattform \"Kaggle\" [T5] verwendet. Kaggle ist eine Plattform der Google LLC und bietet eine Vielzahl von Datensätzen und Modellen für Machine Learning und Data Science Projekte aber auch Wettbewerbe und Kurse an. An dieser Stelle sei noch auf die Plattform \"Hugging Face\" [T6] erwähnt, die ebenfalls eine Vielzahl von Modellen und Datensätzen für Machine Learning und Data Science Projekte anbietet.\n",
    "\n",
    "Der verwendeten Daten \"Gold and Silver prices (2013-2024)\" beinhaltet den täglichen Goldpreis von 2013 bis zum November 2024. Der Datensatz wurde von Alexander Kapturov unter der CC0: Public Domain Lizenz veröffentlicht, wodurch er für alle Zwecke frei verwendet werden kann.\n",
    "\n",
    "Der Datensatz wird als CSV-Datei bereitgestellt und beinhaltet folgende Spalten:\n",
    "* Date: Das Datum des Datensatzes im Format mm/DD/yyyy\n",
    "* Close/Last: Der Schlusskurs des Goldpreises in US-Dollar pro Unze\n",
    "* Volume: Das gehandelte Volumen des Goldes in Unzen\n",
    "* Open: Der Eröffnungskurs des Goldpreises in US-Dollar pro Unze\n",
    "* High: Der höchste Kurs des Goldpreises an diesem Tag in US-Dollar pro Unze\n",
    "* Low: Der niedrigste Kurs des Goldpreises an diesem Tag in US-Dollar pro Unze\n",
    "\n",
    "Für die weitere Analyse wird der Datensatz in ein Pandas DataFrame geladen und die Spalte \"Date\" als Index gesetzt. Anschließend wird der Datensatz visualisiert, um einen ersten Eindruck zu erhalten."
   ],
   "id": "89ff8acc01023e34"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T17:30:09.038171Z",
     "start_time": "2025-01-03T17:30:08.194451Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('Data/gold_prices.csv')\n",
    "df.describe()"
   ],
   "id": "183f590768c3003",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/gold_prices.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the dataset\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mData/gold_prices.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m df\u001B[38;5;241m.\u001B[39mdescribe()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001B[0m, in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[1;32m    899\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[1;32m    900\u001B[0m     dialect,\n\u001B[1;32m    901\u001B[0m     delimiter,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    908\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    909\u001B[0m )\n\u001B[1;32m    910\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[0;32m--> 912\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001B[0m, in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    574\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[1;32m    576\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[0;32m--> 577\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    579\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[1;32m    580\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m   1404\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m   1406\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1407\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[0;34m(self, f, engine)\u001B[0m\n\u001B[1;32m   1659\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[1;32m   1660\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1661\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1662\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1663\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1664\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1665\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1666\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1667\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1668\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1669\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1670\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1671\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1672\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:859\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    854\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    855\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[1;32m    856\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[1;32m    857\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[1;32m    858\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[0;32m--> 859\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    860\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    861\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    862\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    863\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    865\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    866\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    867\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[1;32m    868\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Data/gold_prices.csv'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Quellen:\n",
    "* T4: https://www.kaggle.com/datasets/kapturovalexander/gold-and-silver-prices-2013-2023, [Online, Stand: 03.01.2025]\n",
    "* T5: https://www.kaggle.com/, [Online, Stand: 03.01.2025]\n",
    "* T6: https://huggingface.co/, [Online, Stand: 03.01.2025]"
   ],
   "id": "71a0ac98b8c68268"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Implementierung eines RNN",
   "id": "4e1b7caefc4fc3ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Aktuelle Modelle\n",
    "Beispielmodelle Goldpreis oder Aktienpreis"
   ],
   "id": "c4697ef4fcd6ea3f"
  },
  {
   "cell_type": "markdown",
   "id": "353a46789790b6e9",
   "metadata": {},
   "source": [
    "## 7. Vergleich der Modelle\n",
    "Vergleich mit transformer-netwerken?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61f09d9",
   "metadata": {},
   "source": [
    "## 8. Fazit\n",
    "Eigenes Modell vs. verglichene Standardmodelle."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
